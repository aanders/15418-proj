<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">ul.lst-kix_kczxayf169it-5{list-style-type:none}ul.lst-kix_kczxayf169it-4{list-style-type:none}.lst-kix_gzpq6ha034fx-5>li:before{content:"\0025a0  "}.lst-kix_gzpq6ha034fx-6>li:before{content:"\0025cf  "}ul.lst-kix_kczxayf169it-3{list-style-type:none}ul.lst-kix_kczxayf169it-2{list-style-type:none}.lst-kix_gzpq6ha034fx-4>li:before{content:"\0025cb  "}ul.lst-kix_kczxayf169it-8{list-style-type:none}.lst-kix_gzpq6ha034fx-8>li:before{content:"\0025a0  "}ul.lst-kix_kczxayf169it-7{list-style-type:none}ul.lst-kix_kczxayf169it-6{list-style-type:none}.lst-kix_gzpq6ha034fx-2>li:before{content:"\0025a0  "}.lst-kix_ody52onqwkpx-3>li:before{content:"\0025cf  "}ul.lst-kix_kczxayf169it-1{list-style-type:none}ul.lst-kix_kczxayf169it-0{list-style-type:none}.lst-kix_ody52onqwkpx-4>li:before{content:"\0025cb  "}.lst-kix_gzpq6ha034fx-3>li:before{content:"\0025cf  "}.lst-kix_ody52onqwkpx-5>li:before{content:"\0025a0  "}.lst-kix_gzpq6ha034fx-0>li:before{content:"\0025cf  "}.lst-kix_ody52onqwkpx-6>li:before{content:"\0025cf  "}.lst-kix_gzpq6ha034fx-1>li:before{content:"\0025cb  "}.lst-kix_ody52onqwkpx-7>li:before{content:"\0025cb  "}.lst-kix_ody52onqwkpx-8>li:before{content:"\0025a0  "}ul.lst-kix_ijcoegtsbrql-2{list-style-type:none}ul.lst-kix_ijcoegtsbrql-3{list-style-type:none}ul.lst-kix_ijcoegtsbrql-0{list-style-type:none}ul.lst-kix_ijcoegtsbrql-1{list-style-type:none}ul.lst-kix_ijcoegtsbrql-6{list-style-type:none}ul.lst-kix_ijcoegtsbrql-7{list-style-type:none}ul.lst-kix_ijcoegtsbrql-4{list-style-type:none}ul.lst-kix_ijcoegtsbrql-5{list-style-type:none}ul.lst-kix_ody52onqwkpx-4{list-style-type:none}ul.lst-kix_ody52onqwkpx-5{list-style-type:none}ul.lst-kix_ody52onqwkpx-6{list-style-type:none}ul.lst-kix_ody52onqwkpx-7{list-style-type:none}ul.lst-kix_ody52onqwkpx-8{list-style-type:none}ul.lst-kix_gzpq6ha034fx-0{list-style-type:none}ul.lst-kix_ody52onqwkpx-0{list-style-type:none}ul.lst-kix_ody52onqwkpx-1{list-style-type:none}ul.lst-kix_ody52onqwkpx-2{list-style-type:none}ul.lst-kix_ody52onqwkpx-3{list-style-type:none}ul.lst-kix_gzpq6ha034fx-5{list-style-type:none}ul.lst-kix_gzpq6ha034fx-6{list-style-type:none}ul.lst-kix_gzpq6ha034fx-7{list-style-type:none}ul.lst-kix_gzpq6ha034fx-8{list-style-type:none}ul.lst-kix_gzpq6ha034fx-1{list-style-type:none}ul.lst-kix_gzpq6ha034fx-2{list-style-type:none}ul.lst-kix_gzpq6ha034fx-3{list-style-type:none}ul.lst-kix_gzpq6ha034fx-4{list-style-type:none}ul.lst-kix_e595sgwxr9p7-1{list-style-type:none}ul.lst-kix_e595sgwxr9p7-0{list-style-type:none}ul.lst-kix_e595sgwxr9p7-7{list-style-type:none}ul.lst-kix_m6452g2on6ay-1{list-style-type:none}ul.lst-kix_e595sgwxr9p7-6{list-style-type:none}ul.lst-kix_m6452g2on6ay-2{list-style-type:none}ul.lst-kix_m6452g2on6ay-3{list-style-type:none}.lst-kix_e595sgwxr9p7-0>li:before{content:"\0025cf  "}ul.lst-kix_e595sgwxr9p7-8{list-style-type:none}ul.lst-kix_m6452g2on6ay-4{list-style-type:none}ul.lst-kix_e595sgwxr9p7-3{list-style-type:none}ul.lst-kix_e595sgwxr9p7-2{list-style-type:none}ul.lst-kix_e595sgwxr9p7-5{list-style-type:none}ul.lst-kix_e595sgwxr9p7-4{list-style-type:none}ul.lst-kix_m6452g2on6ay-0{list-style-type:none}.lst-kix_kczxayf169it-8>li:before{content:"\0025a0  "}.lst-kix_kczxayf169it-7>li:before{content:"\0025cb  "}.lst-kix_kczxayf169it-6>li:before{content:"\0025cf  "}.lst-kix_kczxayf169it-4>li:before{content:"\0025cb  "}.lst-kix_kczxayf169it-3>li:before{content:"\0025cf  "}.lst-kix_kczxayf169it-5>li:before{content:"\0025a0  "}ul.lst-kix_m6452g2on6ay-5{list-style-type:none}ul.lst-kix_m6452g2on6ay-6{list-style-type:none}ul.lst-kix_m6452g2on6ay-7{list-style-type:none}.lst-kix_e595sgwxr9p7-1>li:before{content:"\0025cb  "}ul.lst-kix_m6452g2on6ay-8{list-style-type:none}.lst-kix_kczxayf169it-0>li:before{content:"\0025cf  "}.lst-kix_ijcoegtsbrql-0>li:before{content:"\0025cf  "}.lst-kix_kczxayf169it-1>li:before{content:"\0025cb  "}.lst-kix_e595sgwxr9p7-2>li:before{content:"\0025a0  "}.lst-kix_kczxayf169it-2>li:before{content:"\0025a0  "}.lst-kix_ijcoegtsbrql-2>li:before{content:"\0025a0  "}.lst-kix_e595sgwxr9p7-3>li:before{content:"\0025cf  "}.lst-kix_ijcoegtsbrql-1>li:before{content:"\0025cb  "}.lst-kix_e595sgwxr9p7-5>li:before{content:"\0025a0  "}.lst-kix_e595sgwxr9p7-4>li:before{content:"\0025cb  "}.lst-kix_e595sgwxr9p7-6>li:before{content:"\0025cf  "}.lst-kix_e595sgwxr9p7-8>li:before{content:"\0025a0  "}.lst-kix_ijcoegtsbrql-6>li:before{content:"\0025cf  "}.lst-kix_ijcoegtsbrql-8>li:before{content:"\0025a0  "}.lst-kix_mwfnjd22r281-0>li:before{content:"\0025cf  "}.lst-kix_m6452g2on6ay-0>li:before{content:"\0025cf  "}.lst-kix_e595sgwxr9p7-7>li:before{content:"\0025cb  "}.lst-kix_ijcoegtsbrql-5>li:before{content:"\0025a0  "}.lst-kix_mwfnjd22r281-3>li:before{content:"\0025cf  "}.lst-kix_ijcoegtsbrql-4>li:before{content:"\0025cb  "}.lst-kix_mwfnjd22r281-2>li:before{content:"\0025a0  "}.lst-kix_m6452g2on6ay-4>li:before{content:"\0025cb  "}.lst-kix_ijcoegtsbrql-3>li:before{content:"\0025cf  "}.lst-kix_mwfnjd22r281-1>li:before{content:"\0025cb  "}.lst-kix_m6452g2on6ay-5>li:before{content:"\0025a0  "}.lst-kix_m6452g2on6ay-3>li:before{content:"\0025cf  "}.lst-kix_m6452g2on6ay-2>li:before{content:"\0025a0  "}.lst-kix_ijcoegtsbrql-7>li:before{content:"\0025cb  "}.lst-kix_m6452g2on6ay-1>li:before{content:"\0025cb  "}ul.lst-kix_ijcoegtsbrql-8{list-style-type:none}ul.lst-kix_mwfnjd22r281-8{list-style-type:none}.lst-kix_mwfnjd22r281-6>li:before{content:"\0025cf  "}.lst-kix_mwfnjd22r281-8>li:before{content:"\0025a0  "}.lst-kix_m6452g2on6ay-6>li:before{content:"\0025cf  "}.lst-kix_m6452g2on6ay-8>li:before{content:"\0025a0  "}ul.lst-kix_mwfnjd22r281-5{list-style-type:none}ul.lst-kix_mwfnjd22r281-4{list-style-type:none}ul.lst-kix_mwfnjd22r281-7{list-style-type:none}ul.lst-kix_mwfnjd22r281-6{list-style-type:none}.lst-kix_mwfnjd22r281-7>li:before{content:"\0025cb  "}.lst-kix_m6452g2on6ay-7>li:before{content:"\0025cb  "}ul.lst-kix_mwfnjd22r281-1{list-style-type:none}ul.lst-kix_mwfnjd22r281-0{list-style-type:none}ul.lst-kix_mwfnjd22r281-3{list-style-type:none}ul.lst-kix_mwfnjd22r281-2{list-style-type:none}.lst-kix_mwfnjd22r281-4>li:before{content:"\0025cb  "}.lst-kix_mwfnjd22r281-5>li:before{content:"\0025a0  "}.lst-kix_ody52onqwkpx-2>li:before{content:"\0025a0  "}.lst-kix_ody52onqwkpx-1>li:before{content:"\0025cb  "}.lst-kix_gzpq6ha034fx-7>li:before{content:"\0025cb  "}.lst-kix_ody52onqwkpx-0>li:before{content:"\0025cf  "}ol{margin:0;padding:0}table td,table th{padding:0}.c10{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.c12{padding:0;margin:0}.c0{orphans:2;widows:2}.c9{margin-left:36pt;padding-left:0pt}.c7{font-size:14pt}.c5{font-size:10pt}.c13{font-style:italic}.c6{text-align:left}.c8{font-family:"Courier New"}.c1{text-align:center}.c2{color:#0000ff}.c4{font-weight:bold}.c11{font-size:12pt}.c3{height:11pt}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c10"><p class="c0 c1"><span class="c4 c7">Improving Sequential Programs with Multithreaded Data Structure</span></p><p class="c0 c1"><span class="c7">Aaron Anderson &amp; Justus Hibshman</span></p><p class="c0 c1"><span class="c11">15-418 Final Project Writeup</span></p><p class="c0 c3"><span class="c4"></span></p><p class="c0"><span class="c4">Summary:</span></p><p class="c0"><span>As a handy utility, we created a &ldquo;Sorted Collection&rdquo; data structure utilizing parallelism. The data structure is actually composed of two data structures (tree-based and array-based) each maintained behind the scenes by their own thread. Lookups are serviced by the array if possible, but by the tree if necessary. We analyzed our code&rsquo;s performance running on multiple machines and a </span><span>wide variety of</span><span class="c2">&nbsp;</span><span>trace files we generated, </span><span>using a single-threaded tree-based structure as a general reference point</span><span>.</span></p><p class="c0 c3"><span></span></p><p class="c0"><span class="c4">Background:</span></p><p class="c0"><span>The sorted collection interface supports inserts (by value), lookups (by relative position; e.g. &ldquo;Give me the 10th smallest element&rdquo;), and deletes (by relative position)</span><span>. Our sorted collection is comprised of both a tree and an array that contain the same data in sorted order. We also wrote trace file generation code and a testing harness, wherein we check the time spent on each data structure operation as well as how many requests are serviced by each particular sub data structure.</span></p><p class="c0 c3"><span></span></p><p class="c0"><span>Sample output:</span></p><p class="c0 c3"><span></span></p><p class="c0"><span class="c5 c8">Serviced from tree: 88465</span></p><p class="c0"><span class="c5 c8">Serviced from array: 11812</span></p><p class="c0"><span class="c5 c8">Times it wasn&#39;t ready: 2538</span></p><p class="c0"><span class="c5 c8">SortedCollection timing took 1.86111 seconds to complete</span></p><p class="c0"><span class="c5 c8">&nbsp;=== Time spent in data structure operations: 0.318005s</span></p><p class="c0"><span class="c5 c8">&nbsp;=== Average insert time: 0.000153901ms</span></p><p class="c0"><span class="c5 c8">&nbsp;=== Average lookup time: 0.000956742ms</span></p><p class="c0"><span class="c5 c8">&nbsp;=== Average delete time: 0.000186919ms</span></p><p class="c0 c3"><span></span></p><p class="c0"><span>The opportunity for parallelism comes from the fact that the data structures don&rsquo;t need to be &ldquo;up-to-date&rdquo; until a lookup is requested. This allows updates to the structures to be handled asynchronously. When a lookup is performed, the main thread needs some way of knowing when one of the structures can handle its request.</span></p><p class="c0 c3"><span></span></p><p class="c0"><span>Differing data structures will have better capacity to handle different types of loads. By maintaining several data structures asynchronously, we are able to handle many different types of loads well. Particularly, we perform especially well when the load changes throughout the runtime of the application.</span></p><p class="c0 c3"><span></span></p><p class="c0"><span class="c4">Approach:</span></p><p class="c0"><span>We used C++ templates to create our sorted collection. Pthreads, along with the standard library&rsquo;s mutexes and condition variables were our tools for parallelism. We ran tests both on our own machines and </span><span>on some Gates machines.</span></p><p class="c0 c3"><span></span></p><p class="c0"><span class="c4">Diagram of Our Implementation&rsquo;s Data Flow:</span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 268.00px;"><img alt="Data Flow.png" src="images/image06.png" style="width: 624.00px; height: 268.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0 c3"><span></span></p><p class="c0"><span>Gray entities are shared data structures. Other colors represent individual threads. The thread making use of the sorted collection (green) puts the modifications it wants to occur in two lock-free queues. The controllers then pull from the queues and perform the operations on the data structure. When the green thread wishes to perform a lookup, it gets data from the controllers concerning what&rsquo;s up-to-date and accesses data as soon as a structure is ready. If it has a choice, it chooses the array for lookups.</span></p><p class="c0 c3"><span></span></p><p class="c0"><span class="c4">The code for the lock-free queues is taken largely from the lecture slides</span><span>&nbsp;with one important modification: The queue&rsquo;s insert function reuses nodes in the reclaim chain instead of mallocing new queue nodes and freeing the reclaim nodes.</span></p><p class="c0 c3"><span></span></p><p class="c0"><span>We tried to keep communication to a minimum as we knew this could be a major bottleneck in our application. The code is entirely lock-free with the exception of an occasional use of a lock in our latest version of the array controller. Synchronous communication only occurs on a lookup, </span><span>as the main thread might need to wait for one of the substructures to service the lookup</span><span>. In the case of inserts and deletes, the communication is handled by work queues which allow asynchronous operations with respect to the main thread.</span></p><p class="c0 c3"><span></span></p><p class="c0"><span>The main thread keeps a counter of how many updates it entered. When it performs a lookup, it checks to see if the array or the tree controller has performed the same number of updates. (The fanciest version of the array has a slightly more involved check for the lookup. More on this later.)</span></p><p class="c0 c3"><span></span></p><p class="c0"><span>The tree structure used is a standard single-threaded red-black tree written by Aaron last year as an independent project. However, we needed to augment it with size information in order to perform lookups by position. The array was the main focus of optimization, because Justus got out of hand and was very interested in the array (partly). We also optimized it in order to help it perform well alongside a </span><span>tree&mdash;both</span><span>&nbsp;to work well in tandem and to better keep up in more update-heavy situations.</span></p><p class="c0 c3"><span></span></p><p class="c0"><span class="c4">Evolution of the Array:</span></p><p class="c0"><span>As aforementioned, we performed a long series of upgrades and optimizations, in order to allow the array to perform faster and make better use of the fact that it runs asynchronously alongside a tree.</span></p><p class="c0 c3"><span></span></p><p class="c0"><span>Optimization 1:</span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 246.67px;"><img alt="Ev1_2.png" src="images/image05.png" style="width: 624.00px; height: 246.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span>First, we moved the data from the beginning of our allocated block to the middle of it. That way, when we insert or delete, we can shift the data from whichever side requires less shifting.</span></p><p class="c0 c3"><span></span></p><p class="c0"><span>Optimization 2:</span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 258.67px;"><img alt="Ev2_2.png" src="images/image00.png" style="width: 624.00px; height: 258.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span>Next, we added an insertion buffer so we could execute insertions in batch, amortizing the cost. The insertion buffer empties periodically and also before a delete operation is performed.</span></p><p class="c0 c3"><span></span></p><p class="c0"><span>Optimization 3:</span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 292.00px;"><img alt="Ev3_2.png" src="images/image04.png" style="width: 624.00px; height: 292.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span>In this third modification, we turned the insert buffer into an insert-and-delete buffer. That way changes don&rsquo;t need to be flushed whenever a delete appears; the buffer can be much larger. This implementation required careful reasoning about indices, the time that updates occur at, and the fact that we flush the updates in non-chronological order. This batching works well in tandem with the tree, because if the array gets behind, it can let the tree handle requests and start using larger batch sizes to catch up.</span></p><p class="c0 c3"><span></span></p><p class="c0"><span>Optimization 4:</span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 290.67px;"><img alt="Ev4_2.png" src="images/image07.png" style="width: 624.00px; height: 290.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span>Next, we added some opportunities to access data even if the array isn&rsquo;t fully up-to-date. We use several pointers to denote regions in the array that will be untouched by the buffered changes. Then, while changes are being flushed out, the pointers move, broadening the accessible ranges until the flush is complete and the whole array is accessible. It is in this version that we used a lock to protect calculations affected by an important jump in the start pointer&rsquo;s position.</span></p><p class="c0 c3"><span></span></p><p class="c0"><span>Finally, we made a hybrid version that switches between using a light version and using the change buffer (with its associated overhead) depending on how the array as a whole is handling the workload.</span></p><p class="c0 c3"><span></span></p><p class="c0"><span class="c4">Testing:</span></p><p class="c0"><span>The tests were performed using a randomly generated sequence (trace) of inserts, deletes, and lookups, according to various distributions, to model different workloads. We also added pauses between instructions to simulate other computation being done, and varied the length of these pauses as well. We tested on both GHC 43 (6-core Xeon W3670) and on a personal laptop (4-core i7-4700MQ). No other applications were running during testing on the laptop.</span></p><p class="c0 c3"><span></span></p><p class="c0"><span>Our main trace distribution types were</span></p><ul class="c12 lst-kix_m6452g2on6ay-0 start"><li class="c0 c9"><span>Uniform&mdash;inserts, lookups, and deletes are equally likely</span></li><li class="c0 c9"><span>Lookup&mdash;weighted heavily toward lookups</span></li><li class="c0 c9"><span>Insert&mdash;weighted heavily toward inserts</span></li><li class="c0 c9"><span>Flip&mdash;alternates between strings of inserts and of lookups at a rate proportional to trace length</span></li></ul><p class="c0 c3"><span></span></p><p class="c0"><span>The trace-runner&rsquo;s pauses (simulating computation) were implemented by a spin-loop with a counter.</span></p><p class="c0 c3"><span></span></p><p class="c0"><span>For benchmarks, we ran each trace on both a single-threaded red-black tree, and an asynchronous red-black tree (where inserts and deletes are queued and handled by a background thread). The code for the trees in the reference timings is the same as the code used in the sorted collection. We compared these reference timings with the timings obtained from our sorted collection for each trace distribution type across various trace lengths and pause times.</span></p><p class="c0 c3"><span></span></p><p class="c0"><span class="c4">Results:</span></p><p class="c0"><span>Results varied widely by trace file. In some cases, our code performed much worse than our tree running single-threaded. In other cases, having the asynchronous component was quite helpful, but the array simply added overhead. In still other cases, our code performed beautifully and far surpassed both the single-threaded and asynchronous trees.</span></p><p class="c0 c3"><span></span></p><p class="c0"><span>Example of Performing Badly:</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 425.33px;"><img alt="insert-total-100.png" src="images/image02.png" style="width: 624.00px; height: 425.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0 c1"><span class="c5">[Delay between ops = 100 int increments]</span></p><p class="c0 c6 c3"><span></span></p><p class="c0 c6"><span>We perform poorly here because the space between data structure operations is so small that the minute benefits we get from asynchronization is overpowered by the overhead induced by our collection. Indeed, we see that as we increase the delay, our data structure gets better and better performance overall.</span></p><p class="c0 c3"><span></span></p><p class="c0"><span>Example of Performing Well:</span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 426.67px;"><img alt="data-structure-ops.png" src="images/image09.png" style="width: 624.00px; height: 426.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0 c1"><span class="c5">[Delay between ops = 1000 int increments]</span></p><p class="c0 c3"><span></span></p><p class="c0"><span>We find the bottom-right of the above 4 graphs to be particularly interesting. In that graph, the sorted collection effectively matches the asynchronous tree in performance. This is an &ldquo;equilibrium point,&rdquo; where the overhead of having the array has been matched by the quick lookups it provides. Roughly 30% of the lookups in that graph are from the array. As the array sees more usage, the sorted collection will pass the async-tree in performance. If this progression continues for a long time, one would eventually be better off with just an asynchronous array, in the same way that some cases perform better with just an asynchronous tree. It is in the middle ground, or with less predictable traces, that our sorted collection is most useful.</span></p><p class="c0 c3"><span></span></p><p class="c0"><span>Example of Performing Beautifully:</span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 428.00px;"><img alt="data-structure-ops-5000.png" src="images/image10.png" style="width: 624.00px; height: 428.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0 c1"><span class="c5">[Delay between ops = 5000 int increments]</span></p><p class="c0 c3"><span></span></p><p class="c0"><span>As evidenced by the above lower-right graph, we can achieve more than 10x speedup over the sequential red-black tree and more than 5x over the asynchronous tree in particularly good cases.</span></p><p class="c0 c3"><span></span></p><p class="c0"><span>We also examined the average performance of each operation independently for the different types of traces. We found that the time spent for inserts and deletes was generally quite small, which we expected since they simply insert operations into the queue. Happily, we also found that in many cases, the lookups are much faster as well. &nbsp;These results are illustrated in the following three graphs.</span></p><p class="c0 c3"><span></span></p><p class="c0 c3"><span></span></p><p class="c0 c3"><span></span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 428.00px;"><img alt="avg-lookup-time.png" src="images/image03.png" style="width: 624.00px; height: 428.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0 c1"><span class="c5">[Delay between ops = 1000 int increments]</span></p><p class="c0 c3"><span></span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 426.67px;"><img alt="avg-insert-time.png" src="images/image08.png" style="width: 624.00px; height: 426.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0 c1"><span class="c5">[Delay between ops = 1000 int increments]</span></p><p class="c0 c3"><span></span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 428.00px;"><img alt="avg-delete-time.png" src="images/image11.png" style="width: 624.00px; height: 428.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0 c1"><span class="c5">[Delay between ops = 1000 int increments]</span></p><p class="c0 c6 c3"><span></span></p><p class="c0"><span>Generally, our inserts and deletes were very fast. However, in traces with short pauses and insert-heavy portions, the queue could start to take longer. Our belief is that this stems from the fact that as a queue gets seriously behind, the number of reclaim nodes available for reuse by the queue starts to dwindle, and calls to queue-&gt;insert() need to start allocating new queue nodes. These allocations are expensive. Note the two bottom graphs below:</span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 429.33px;"><img alt="avg-insert-time-100.png" src="images/image01.png" style="width: 624.00px; height: 429.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0 c1"><span class="c5">[Delay between ops = 100 int increments]</span></p><p class="c0 c3"><span></span></p><p class="c0"><span>To check if we were bandwidth bound, we ran an extra thread that used up memory bandwidth (calls to memcpy). Performance was minimally affected. This, of course, was for the machine we tested on. Given that our code is intended for a variety of systems, it doesn&rsquo;t make complete sense to say &ldquo;it&rsquo;s bandwidth bound&rdquo; or &ldquo;it&rsquo;s compute bound&rdquo; because that will vary largely depending on the system which uses our code.</span></p><p class="c0 c6 c3"><span></span></p><p class="c0 c6"><span>We note that there are several environments in which our code is most useful:</span></p><ul class="c12 lst-kix_ijcoegtsbrql-0 start"><li class="c0 c9 c6"><span>The machine has sufficient resources (cores, bandwidth)</span></li><li class="c0 c9 c6"><span>Application </span><span class="c13">occasionally</span><span>&nbsp;wants to perform data structure operations </span><span class="c13">very quickly</span></li><li class="c0 c9 c6"><span>The workload on the data structure fluctuates throughout the run, allowing full use of both the tree and the array</span></li></ul><p class="c0 c6 c3"><span></span></p><p class="c0 c6"><span>It is important to note that the machine having sufficient resources is quite important for good performance. Aaron&rsquo;s laptop has an i5 processor with only two cores&mdash;and thus only two sets of execution resources (ALUs, etc). With three threads total in the program (the main thread plus each substructure controller), not everything was able to run simultaneously, leading to a degradation in performance. When moving to a machine with more cores, the performance improved dramatically.</span></p><p class="c0 c6 c3"><span></span></p><p class="c0"><span class="c4">List of Work by Each Student:</span></p><p class="c0"><span>Equal work was performed by both project members. </span></p></body></html>
